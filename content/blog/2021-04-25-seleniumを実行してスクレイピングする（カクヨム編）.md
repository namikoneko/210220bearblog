---
title: seleniumを実行してスクレイピングする（カクヨム編）
slug: execute-selenium-kakuyomu
date: 2021-04-25T00:33:35.297Z
description: seleniumを実行してカクヨムをスクレイピングする。
tags:
  - python
  - scraping
---
## ディレクトリ

/Users/takashi/Downloads/python/210110

成功した

## エピソードを登録するコード

```python
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.keys import Keys
import time

#driver = webdriver.Chrome('/Users/takashi/Downloads/python/201229/chromedriver')
options = webdriver.ChromeOptions()
options.add_argument('--headless')
#options.add_argument('--no-sandbox')
driver = webdriver.Chrome(executable_path='/Users/takashi/Downloads/python/210110/chromedriver',options=options)

# 要素が見つかるまで、最大20秒間待機する
driver.implicitly_wait(20)

#driver.get('https://www.google.co.jp/') 
driver.get('https://kakuyomu.jp/login?location=%2F')

time.sleep(2)
#<input type="email" name="email_address" placeholder="メールアドレス" aria-label="メールアドレス" tabindex="1" required="" autofocus="">
#<input type="password" name="password" placeholder="パスワード" aria-label="パスワード" tabindex="2" required="">
search_box = driver.find_element_by_name('email_address')
search_box.send_keys('e-mail-address')
search_box = driver.find_element_by_name('password')
search_box.send_keys('secret-password')
search_box.submit()
time.sleep(2)

#driver.find_element_by_xpath('//*[@id="globalHeaderPC-subNav"]/li[2]/a/span').click()#ヘッドレスモードではなぜかできなかった
#driver.find_element_by_link_text('ワークスペース').click()#うまくいかなかった
driver.get('https://kakuyomu.jp/my')#直接urlをgetしたら遷移できた

driver.find_element_by_link_text('aaa').click()
#小説aaaのurlに直接遷移してもいい
print(driver.current_url)
time.sleep(2)

for i in range(3):

    #次のエピソードを執筆
    driver.find_element_by_xpath('//*[@id="toc-bulkEditForm"]/div[1]/p/a').click()
    #print(driver.current_url)
    time.sleep(2)

    textbox = driver.find_element_by_xpath('//*[@id="episodeBody-input"]')
    textbox.send_keys("selenium!1024")
    textbox.submit()
    #print(driver.current_url)
    time.sleep(2)

driver.close()
driver.quit()

```

## selenium+pythonを使ってテキストのリンクURLを取得する

<https://qiita.com/pistachio1000g/items/a8cc7842468d649e2769>

```html
<p id="id_hoge"><a href="https://hogehoge.html" target="_blank">hoogehoge</a></p>
```

```python
driver.get('取得したいページのURL')
element = driver.find_element_by_id('id_hoge')
aTag    = element.find_element_by_tag_name("a")
url     = aTag.get_attribute("href")
```

## 小説の各話のurlを取得するコード

```python
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.keys import Keys
from bs4 import BeautifulSoup#使わないけれど
import time

options = webdriver.ChromeOptions()
options.add_argument('--headless')
#options.add_argument('--no-sandbox')
driver = webdriver.Chrome(executable_path='/Users/takashi/Downloads/python/210110/chromedriver',options=options)

# 要素が見つかるまで、最大20秒間待機する
driver.implicitly_wait(20)

#ログイン画面にアクセス
driver.get('https://kakuyomu.jp/login?location=%2F')
time.sleep(2)

#メアド、パスワードを入力
search_box = driver.find_element_by_name('email_address')
search_box.send_keys('mailAddress')
search_box = driver.find_element_by_name('password')
search_box.send_keys('secret-password')
search_box.submit()
time.sleep(2)

#マイページへ遷移して、小説名のリンクテキストaaaをクリック
driver.get('https://kakuyomu.jp/my')
driver.find_element_by_link_text('aaa').click()

#各話のurlを取得
elements = driver.find_elements_by_class_name("episode-edit")
i = 0
for element in elements:
    i = i + 1
    aTag = element.find_element_by_tag_name("a")
    url  = aTag.get_attribute("href")
    print(i)
    print(url)

driver.close()
driver.quit()
```

## csvに書き出すコード（追加分）

```python
import csv#csv用
import os.path#csv用

#csv用、forを回す直前
dirname = os.path.dirname(__file__)
path = os.path.join(dirname, "test.csv")
f = open(path, "w", encoding="utf-8", newline="")
writer = csv.writer(f)

    writer.writerow([i, url])#csv用、forの中

f.close()#csv用、forの終了直後
```

## csvから読み込むコード
```python
import csv 
import os.path

dirname = os.path.dirname(__file__)
path = os.path.join(dirname, "test.csv")
f = open(path, "r", encoding="utf-8")
reader = csv.reader(f)
urls = []
for l in reader:
    #print(l)
    urls.append(l)

f.close()
print(urls)
```

## 想定している流れ

paras.csvにコンテンツを書く

体力点だけが違うパラグラフなどは，ループでパラグラフを追加することもある

リンクは、to-goburin-roomなどとする

`text.py`のpythonでcsvを読み込んで、pythonの中でリストを作る

shuffleする

shuffle後の、パラグラフナンバーのリストを作る

urlsのリストを作る（csvから読み込む、先にエピソードをパラグラフ数だけ作り、urlをgetして，csvに追記する）

to-goburin-roomなどが，パラグラフナンバーのリストの何番目か調べ，これを使ってurlsのリストから，urlを取得して置換する

```
urls[パラグラフナンバーのリストの何番目か]
```

リンクを直したコンテンツを、スクレイピングでカクヨムに書き込む

## 想定している流れ210501

twinにコンテンツを書く

１つのアイテムがあるかないか、0か1のフラグ管理に留める

体力は、スタート地点に戻ると回復するようにする

ファイルにコンテンツを移す

カクヨムのエピソードを作り、urlをcsvに記録する

コンテンツファイルのファイル名、内容をリストにしてshuffle、ファイル名を手がかりに、何番目に移動したかを検索、その順番で、urslのリストからurlを取得、置換する
そして書き込む
